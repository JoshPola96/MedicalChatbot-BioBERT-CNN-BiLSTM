{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 10422145,
          "sourceType": "datasetVersion",
          "datasetId": 6459741
        }
      ],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def setup_gpu_strategy():\n",
        "    try:\n",
        "        # Configure GPU memory growth\n",
        "        physical_devices = tf.config.list_physical_devices('GPU')\n",
        "        if physical_devices:\n",
        "            for device in physical_devices:\n",
        "                try:\n",
        "                    tf.config.experimental.set_memory_growth(device, True)\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"Could not set memory growth for {device.name}: {e}\")\n",
        "            print(f\"Found {len(physical_devices)} GPU(s). GPU configuration successful.\")\n",
        "\n",
        "            # Create and return GPU strategy\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "            print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
        "            return strategy\n",
        "        else:\n",
        "            print(\"No GPUs found. Falling back to CPU strategy.\")\n",
        "            return tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU configuration failed: {e}\")\n",
        "        print(\"Falling back to default strategy.\")\n",
        "        return tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")"
      ],
      "metadata": {
        "id": "5j88RJEDtTqc",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import logging\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import keras_tuner as kt\n",
        "from transformers import AutoConfig, TFBertModel"
      ],
      "metadata": {
        "id": "gpQ3d1S62Cuu",
        "outputId": "cb1ae98d-c1d2-4121-ebdf-689dc4144340",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.11/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2024.12.14)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration Constants\n",
        "FILE_NAME = '/content/chatbot-med-df.tfrecord'\n",
        "MAX_LENGTH = 256\n",
        "BATCH_SIZE = 1\n",
        "SUBSET_SIZE = 100\n",
        "VAL_SPLIT = 0.1\n",
        "TEST_SPLIT = 0.1\n",
        "NUM_EPOCHS = 100\n",
        "BERT_MODEL_NAME = 'dmis-lab/biobert-base-cased-v1.1'\n",
        "MODEL_SAVE_PATH = '/saved_model'"
      ],
      "metadata": {
        "id": "SUtaWOSl0o5-",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset_for_tuning(filename, batch_size, subset_size=None, val_split=0.1):\n",
        "    \"\"\"\n",
        "    Load and split the dataset into training and validation datasets, tailored for hyperparameter tuning.\n",
        "    \"\"\"\n",
        "\n",
        "    # Feature description for parsing the TFRecord\n",
        "    feature_description = {\n",
        "        'query_input_ids': tf.io.FixedLenFeature([MAX_LENGTH], tf.int64),\n",
        "        'query_attention_mask': tf.io.FixedLenFeature([MAX_LENGTH], tf.int64),\n",
        "        'response_input_ids': tf.io.FixedLenFeature([MAX_LENGTH], tf.int64)\n",
        "    }\n",
        "\n",
        "    def parse_example(example):\n",
        "        parsed = tf.io.parse_single_example(example, feature_description)\n",
        "\n",
        "        # Define inputs and targets\n",
        "        inputs = {\n",
        "            'query_input_ids': tf.cast(parsed['query_input_ids'], tf.int32),\n",
        "            'query_attention_mask': tf.cast(parsed['query_attention_mask'], tf.int32),\n",
        "        }\n",
        "        targets = tf.cast(parsed['response_input_ids'], tf.int32)\n",
        "        return inputs, targets\n",
        "\n",
        "    # Load the TFRecord dataset\n",
        "    raw_dataset = tf.data.TFRecordDataset([filename])\n",
        "    parsed_dataset = raw_dataset.map(parse_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # Determine dataset size\n",
        "    total_size = sum(1 for _ in parsed_dataset)\n",
        "    print(f\"Original dataset size: {total_size}\")\n",
        "\n",
        "    # Subset handling\n",
        "    if subset_size and subset_size < total_size:\n",
        "        total_size = subset_size\n",
        "        parsed_dataset = parsed_dataset.take(subset_size)\n",
        "        print(f\"Taking subset of size: {subset_size}\")\n",
        "\n",
        "    # Calculate split sizes\n",
        "    train_size = int(total_size * (1 - val_split))\n",
        "    val_size = total_size - train_size\n",
        "\n",
        "    # Splits\n",
        "    train_dataset = parsed_dataset.take(train_size).shuffle(buffer_size=10000)\n",
        "    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    val_dataset = parsed_dataset.skip(train_size)\n",
        "    val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    print(f\"Dataset successfully loaded for tuning:\")\n",
        "    print(f\"  - Total size: {total_size}\")\n",
        "    print(f\"  - Train size: {train_size}\")\n",
        "    print(f\"  - Validation size: {val_size}\")\n",
        "\n",
        "    # Return only the training and validation datasets, along with their sizes\n",
        "    return train_dataset, val_dataset, train_size, val_size"
      ],
      "metadata": {
        "id": "MOTFSS464On7",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
        "class BioBertEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, bert_model_name, trainable=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.bert_model_name = bert_model_name\n",
        "        self.trainable = trainable\n",
        "        self.bert_model = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Initialize the BERT model\n",
        "        self.bert_config = AutoConfig.from_pretrained(self.bert_model_name)\n",
        "        self.bert_model = TFBertModel.from_pretrained(\n",
        "            self.bert_model_name, config=self.bert_config, from_pt=True\n",
        "        )\n",
        "        self.bert_model.trainable = self.trainable\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        input_ids, attention_mask = inputs\n",
        "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask, training=training)\n",
        "        return outputs.last_hidden_state\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"bert_model_name\": self.bert_model_name, \"trainable\": self.trainable})\n",
        "        return config"
      ],
      "metadata": {
        "id": "5s-jvZKeAGBJ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
        "def masked_loss(y_true, y_pred):\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "    loss *= mask\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
        "def masked_loss_metric(y_true, y_pred):\n",
        "    # Convert the loss function into a metric\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "    loss *= mask\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "-Hppzpc3bMQ6",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "class BioBertCnnBiLSTM:\n",
        "    def __init__(self, bert_model_name, vocab_size):\n",
        "        self.bert_model_name = bert_model_name\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build(self, config):\n",
        "        # Define inputs\n",
        "        query_input_ids = tf.keras.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='query_input_ids')\n",
        "        query_attention_mask = tf.keras.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='query_attention_mask')\n",
        "\n",
        "        bert_layer = BioBertEncoder(self.bert_model_name, trainable=False)\n",
        "\n",
        "        query_bert_output = bert_layer([query_input_ids, query_attention_mask])\n",
        "\n",
        "        # CNN Layer\n",
        "        kernel_sizes = config['kernels']\n",
        "        query_cnn = []\n",
        "        for kernel_size in kernel_sizes:\n",
        "            conv = tf.keras.layers.Conv1D(\n",
        "                filters=config['cnn_filters'],\n",
        "                kernel_size=kernel_size,\n",
        "                padding='same',\n",
        "                activation=config['activation_fn'],\n",
        "                kernel_regularizer=l2(config['cnn_regularization'])\n",
        "            )(query_bert_output)\n",
        "            bn = tf.keras.layers.BatchNormalization()(conv)\n",
        "            dropout = tf.keras.layers.Dropout(config['dropout_cnn'])(bn)\n",
        "            query_cnn.append(dropout)\n",
        "\n",
        "        query_cnn = tf.keras.layers.Concatenate()(query_cnn)\n",
        "\n",
        "        # LSTM Layer\n",
        "        query_lstm = tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(config['lstm_units'], dropout=config['dropout_lstm'], return_sequences=True)\n",
        "        )(query_cnn)\n",
        "\n",
        "        # Output Layer\n",
        "        output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.vocab_size, activation='softmax'))(query_lstm)\n",
        "\n",
        "        model = tf.keras.Model(inputs={'query_input_ids': query_input_ids, 'query_attention_mask': query_attention_mask}, outputs=output)\n",
        "\n",
        "        # Optimizer with learning rate schedule\n",
        "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            initial_learning_rate=config['learning_rate'],\n",
        "            decay_steps=100000,\n",
        "            decay_rate=0.96,\n",
        "            staircase=True\n",
        "        )\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "        model.compile(optimizer=optimizer, loss=masked_loss, metrics=[masked_loss_metric, 'accuracy'])\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "TKIz6QsfbYfK",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "    config = {\n",
        "        'kernels': [hp.Choice('kernels', values=[3, 5, 7])],\n",
        "        'cnn_filters': hp.Int('cnn_filters', min_value=32, max_value=128, step=32),\n",
        "        'activation_fn': hp.Choice('activation_fn', values=['relu', 'tanh', 'swish']),\n",
        "        'cnn_regularization': hp.Float('cnn_regularization', min_value=0.01, max_value=0.1, step=0.01),\n",
        "        'dropout_cnn': hp.Float('dropout_cnn', min_value=0.1, max_value=0.5, step=0.1),\n",
        "        'lstm_units': hp.Int('lstm_units', min_value=32, max_value=128, step=32),\n",
        "        'dropout_lstm': hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1),\n",
        "        'learning_rate': hp.Float('learning_rate', min_value=1e-5, max_value=1e-3, sampling='log'),\n",
        "    }\n",
        "\n",
        "    model = BioBertCnnBiLSTM(bert_model_name=BERT_MODEL_NAME, vocab_size=28996)\n",
        "    return model.build(config)"
      ],
      "metadata": {
        "trusted": true,
        "id": "O7gqKwC5ZM6v"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_hyperparameters(file_path):\n",
        "    \"\"\"\n",
        "    Tune the model's hyperparameters using Keras Tuner.\n",
        "    \"\"\"\n",
        "    # 2.1. Load and split dataset\n",
        "    print(\"Loading datasets...\")\n",
        "    train_dataset, val_dataset, train_size, val_size = load_dataset_for_tuning(\n",
        "        filename=file_path,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        subset_size=SUBSET_SIZE,\n",
        "        val_split=VAL_SPLIT\n",
        "    )\n",
        "\n",
        "    # 2.2. Set up Keras Tuner with Hyperband search\n",
        "    tuner = kt.Hyperband(\n",
        "        build_model,\n",
        "        objective=kt.Objective('masked_loss_metric', direction='min'),\n",
        "        max_epochs=NUM_EPOCHS,\n",
        "        factor=3,\n",
        "        hyperband_iterations=1,\n",
        "        directory='tuner_dir',\n",
        "        project_name='chatbot_tuning'\n",
        "    )\n",
        "\n",
        "    # 2.3. Start hyperparameter tuning search\n",
        "    tuner.search(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=NUM_EPOCHS,\n",
        "        steps_per_epoch=train_size // BATCH_SIZE,\n",
        "        validation_steps=val_size // BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # 2.4. Get best hyperparameters and best model\n",
        "    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    print(f\"Best Hyperparameters: {best_hp.values}\")\n",
        "\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    # 2.5. Evaluate the best model on the test set\n",
        "    test_results = best_model.evaluate(test_dataset)\n",
        "    print(f\"Test results: Loss = {test_results[0]:.4f}, Accuracy = {test_results[1]:.4f}\")\n",
        "\n",
        "    return best_model, test_results"
      ],
      "metadata": {
        "id": "pfILYLN8phsU",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "    logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "    file_path = FILE_NAME\n",
        "\n",
        "    best_model, test_results = tune_hyperparameters(file_path)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjU6RqqIZM6x",
        "outputId": "10caff2c-959b-44fd-b4d8-e2beff3b3208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 64 Complete [00h 02m 43s]\n",
            "masked_loss_metric: 0.0\n",
            "\n",
            "Best masked_loss_metric So Far: 0.0\n",
            "Total elapsed time: 03h 17m 36s\n",
            "\n",
            "Search: Running Trial #65\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "7                 |5                 |kernels\n",
            "96                |32                |cnn_filters\n",
            "tanh              |relu              |activation_fn\n",
            "0.1               |0.01              |cnn_regularization\n",
            "0.5               |0.2               |dropout_cnn\n",
            "64                |128               |lstm_units\n",
            "0.2               |0.4               |dropout_lstm\n",
            "4.9007e-05        |3.3907e-05        |learning_rate\n",
            "17                |6                 |tuner/epochs\n",
            "6                 |0                 |tuner/initial_epoch\n",
            "2                 |2                 |tuner/bracket\n",
            "1                 |0                 |tuner/round\n",
            "0049              |None              |tuner/trial_id\n",
            "\n",
            "Epoch 7/17\n",
            "\u001b[1m26/90\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 163ms/step - accuracy: 4.5255e-04 - loss: 21.0138 - masked_loss_metric: 10.2567"
          ]
        }
      ],
      "execution_count": null
    }
  ]
}